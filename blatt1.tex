\documentclass[11pt]{article}
\usepackage[sc]{mathpazo}
\usepackage{geometry}

%\geometry{verbose,tmargin=1cm,bmargin=1.5cm,lmargin=2.5cm,rmargin=3cm}
%\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true, pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}

%\usepackage{breakurl} % gives error in compilation

% Load extra packages for LaTex
\usepackage{amsmath}
\usepackage{bm}
\usepackage{paralist}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{color}

% Umgang mit Datumsangaben
\usepackage{datetime}

%\usepackage[round]{natbib}
\usepackage[abbr]{harvard}

% german stuff
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}

% packages to include text verbatim
\usepackage{verbatim}
\usepackage{listings}

% Beschriftung Graphiken und Tabellen
\usepackage{caption}
\captionsetup[figure]{font=normalsize, labelfont=bf, format=plain, width=0.95\textwidth, skip=-2pt}
%\captionsetup[table]{position=above}
\captionsetup[table]{position=top, font=normalsize, labelfont=bf, format=plain, width=0.95\textwidth}


\input{../header.tex}

\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Vor Beginn des Dokuments folgendes einfügen:

\usepackage{textcomp}
\setlength{\parindent}{0em} \setlength{\textwidth}{17cm}
\setlength{\textheight}{25cm} \setlength{\topmargin}{0cm}
\setlength{\oddsidemargin}{-0.8cm}%linker seitenrand
\setlength{\headheight}{-1cm} \setlength{\headsep}{0cm}
\newcommand{\nat}{{\it I\hspace{-0.15cm}N}}
\newcommand{\reel}{{\it I\hspace{-0.15cm} R}}
\newcommand{\zahlen}{{\it Z\hspace{-0.155cm} Z}}
\newcommand{\ix}{{\it X\hspace{-.4cm}\vspace{-12mm} $\,^\_$}}
%\pagestyle{empty}
\newcommand{\COMMENT}[1]{}

%Schafft verschiedene Umgebungen, die nicht erscheinen, wenn \excludecomment{} im Code steht
%(Umgebungen erscheinen nicht, wenn \excludecomment{} auskommentiert wird)

\usepackage{comment}
\newenvironment{angabe}{}{}           %angabe für Übungsblatt
%\excludecomment{angabe}              %Diese Zeile Auskommentieren=Uebungsblatt
\newcommand{\uba}{ \begin{angabe} }  %Abkürzung für den Anfang einer angabe-Umgebung
\newcommand{\ubf}{ \end{angabe} }    %Abkürzung für das Ende einer angabe-Umgebung

\newenvironment{loesung}{}{}          %loesung für Lösung
%\excludecomment{loesung}              %Diese Zeile Auskommentieren=Lösungsblatt
\newcommand{\la}{\begin{loesung}}    %Abkürzung für den Anfang einer loesung-Umgebung
\newcommand{\lf}{\end{loesung}}      %Abkürzung für das Ende einer loesung-Umgebung



% von Hand ändern, um Blattnummern im Kopf anzupassen
\newcommand{\nummer}{1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}



% statt dem alten Kopf einfach folgende Zeile einfügen
\input{../Kopf}

\vspace{-.4cm}
\begin{center}
    \Large\textbf{Exercise Sheet \nummer}\\
\end{center}
\vspace{-.4cm}


\section*{Tutorial Problems}




{\bf Exercise 1.1 Random Variables}\vspace{0.2cm}

Random variables are a key concept of probability theory.
 



\vspace*{-1em}
\bi
\item[(a)] Give the definition of a probability space as well as the definition of a random variable. 

\begin{loesung}\vspace{0.2cm}
A probability space is a triplet $(\Omega,F,P)$. Here $\Omega$ denotes a sample space, $F$ a sigma algebra (collection of measurable sets) and $P$ a probability measure.\\ 
Given a probability space a random variable $X$ is a measurable function from $\Omega$ on some space of outcomes, e.g. the $\mathbb{R}^{n}$. 
\end{loesung}

\item[(b)] Random variables are usually identified with certain functions: Give the definition of a (cumulative) distribution function and a probability (density) function for a univariate real valued random variable. How are they connected to each other? Distinguish the discrete and continuous case.

\begin{loesung}\vspace{0.2cm}
The (cumulative) distribution function of a random variable $X$ is defined as $F(x)=P(X\leq x)$., with $x\in \mathbb{R}$. The probability function in the discrete case is defined as $f(x)=P(x)$. In the continuous case by contrast it, is defined as a density $f(x)$, which satisfies the equation $P(X\in B)=\int_{B}f(x)dx$ for all measurable sets $B$. In the discrete case, one gets the distribution function from the probability function via summation, in the continuous case via integration. 
\end{loesung}

\item[(c)] Often random variables are defined as functions of other random variables. This is sometimes called change of variable. The following theorem provides a relationship of the densities: 

\vspace*{0.2em}
Let $g$ be an invertible and differentiable function that transforms the random variable $X$ to the random variable $Y=g(X)$, then:
\[ f_Y(y)=\left| \frac{d}{dy}g^{-1}(y)\right|f_X(g^{-1}(y))\]\

Given $X\sim N(\mu,\sigma^2)$ compute the density of $Y=exp(X)$. $Y$ is called lognormal distributed.

\begin{loesung}\vspace{0.2cm}
The transformation function $g(x)=e^x$ is differentiable and invertible. Moreover, the inverse is - remember $y>0$ - $(g^{-1}(y)))=ln(y)$, with the derivative $\frac{d}{dy}g^{-1}(y)=\frac{1}{y}$.
Using the given formula, this results in:

\begin{eqnarray*}
 f_Y(y)= & = & \frac{1}{y} \cdot \frac{1}{\sqrt{2\pi} \sigma}e^{-\frac{(ln(y)-\mu)^2}{2\sigma^2}}\\
\end{eqnarray*}

\end{loesung}
\ei



{\bf Exercise 1.2 Central Limit Theorem}\vspace{0.2cm}

An employee stays on all 225 working days of the year a short time longer in the office than needed. This additional working time on one day is described with an exponential distributed random variable with an expected value of 5 minutes. This means its density is $f_Y(y)=\lambda e^{-\lambda y}$ with some $\lambda$. Moreover, we assume that different days are independent.

\begin{enumerate}
\item[(a)] Use the central limit theorem from the lecture to compute the approximative distribution of the employee's extra working time for a whole year. 

\begin{loesung}\vspace{0.2cm}
\textbf{Central Limit Theorem}: $X_1,\ldots,X_n$ are i.i.d. random variables with 
    \[
 E(X_i) = \mu \quad \textrm{und} \quad Var(X_i)=\sigma^2 > 0.
\]
    Then the distribution function $F_n(z)=P(Z_n \leq z)$ of the standardized sum 
    \[
Z_n = \frac{X_1 + \ldots + X_n - n\mu}{\sqrt{n}\sigma}=\frac{1}{\sqrt{n}}\sum_{i=1}^n \frac{X_i-\mu}{\sigma}
    \]
    converges for $n \rightarrow \infty$ for every point $z \in \mathbb{R}$ to $\Phi(z)$, the standard normal distribution function. This is called convergence in density: 
    \[
Z_n \xrightarrow{d} N(0,1).
    \]
One can also write $Z_n$ has an asymptotic normal distribution:
    \[
Z_n \stackrel{a}{\sim} N(0,1)
    \]
    
Let $X_i$ denote the additional working time on day $i$ with $i=1,\ldots,225$. Then the $X_i\sim Exp(\lambda)$ are identically and independently distributed. The parameter $\lambda$ can be computed with the help of the given expectation:
\begin{eqnarray*}
E(X_i) = 5 [\text{min}] & = & \frac{1}{\lambda} \\
                 & \Leftrightarrow & \lambda = 0.2, \\
                 Var(X_i) = \frac{1}{\lambda^2} & = & \frac{1}{0.04} = 25.
\end{eqnarray*}
    Let now $X := \sum_{i=1}^{225}X_i$ be the annual additional working time.
    
Using the central limit theorem it holds:
    \[
Z_n  := \frac{\sum_{i=1}^{225} X_i - 225 \cdot 5 }{\sqrt{225} \sqrt{25}} \stackrel{a}{\sim} N (0, 1)
    \]
    And as a result:
    \[
    \sum_{i=1}^{225} X_i \stackrel{a}{\sim} N (225 \cdot 5, 225 \cdot 25)
    \]    
    

\end{loesung}



\item[(b)] Compute the probability that the employee works more than 16 additional hours.



\begin{loesung}\vspace{0.2cm}
It holds $16 [\text{h}] = 16 \cdot 60 [\text{min}] = 960 [\text{min}]$. As a result we are interested in $P(X > 960)$. 
\begin{eqnarray*}
  P(X > 960) &=& 1 - P(X \leq 960)\\
             &=& 1 - P\left(\frac{X - 225 \cdot 5}{15 \cdot 5} \leq \frac{960 - 225 \cdot 5}{15 \cdot 5} \right) \\
             &=& 1 - \Phi (-2.2) \\
             &=& \Phi (2.2) \\
             &\approx& 0.9861.
\end{eqnarray*}
The probability of the employee working more than $16$ additional hours over one year is approximately $0.9861$.

\end{loesung}
\end{enumerate}


{\bf Exercise 1.3 Speed of convergence in the Central Limit Theorem (\R-exercise)}\vspace{0.2cm}\\

In the lecture several plots were presented in order to visualize the Central Limit Theorem. The goal of this exercise is to create some of these plots.

\bi
\item[(a)] Simulate a $Bin(100,0.75)$ variable several times and make a histogram. This variable is asymptotically normal distributed. Overlay the corresponding density function given by the Central Limit Theorem. Make sure you are used to the \R-commands: \texttt{dbinom,pbinom,qbinom,rbinom}.

\item[(b)] Write a function - depending on n and on p - that plots a histogram of the standardized mean $Z_n$ assuming $X_i$ is Bernoulli distributed with parameter $p$ and overlay the standard normal density. Add a QQ-plot and a histogram of $X_i$ next to it.

\item[(c)] We now take a closer look at the Central Limit Theorem for the following distributions of $X_i$: $Ber(0.5),Ber(0.75),Ber(0.9),Ber(0.99)$. Use your function from (b) to make a statement about the speed of convergence and also compare the convergence for the different distributions. What might be general properties that guarantee fast convergence?
\ei



{\bf Exercise 1.4 Moment Generating Functions}\vspace{0.2cm}

Given are two random variables $X \sim Bin(n,p)$ and $Y \sim Exp(\lambda)$,i.e. $Y$ has an exponential distribution with parameter $\lambda$. (Hint: You may want to use the fact that the binomial distribution can be written as the sum of \textbf{independent} Bernoulli random variables.)

\bi
\item[(a)] Calculate the expected value and the variance for both variables. 

\begin{loesung}\vspace{0.2cm}
We use $X=\sum_{i=1}^n X_i$ with $X_i \siid Ber(p)$, i.e. $P(X_i=1)=p$ and $P(X_i=0)=1-p$. Therefore it holds: 


\[\  E(X)  =  E(\sum_{i=1}^n X_i)  =  \sum_{i=1}^n E(X_i) =  np \]\

A similar computation shows for the variance:
\[\  Var(X)  =  Var(\sum_{i=1}^n X_i)  \stackrel{iid}{=}  \sum_{i=1}^n Var(X_i) =  np(1-p) \]\
For $Y$ we make use of the definition:

\begin{eqnarray*}
 E(Y) & = & \int_{0}^{\infty} y f_Y(y)dy = \int_0^{\infty} y \cdot \lambda e^{-\lambda y}dy\\
 &\stackrel{part. int.}{=}& -\left[y \cdot e^{-\lambda y}\right]_0^{\infty} + \int_0^{\infty} e^{-\lambda y}dy
 \\
 & = & 0 - \left[ \frac{1}{\lambda} e^{-\lambda y}\right]_0^{\infty} =\frac{1}{\lambda}
\end{eqnarray*}


For the computation of the variance of $Y$ we use the identity $Var(Y)=E(Y^2)-E(Y)^2$.

\begin{eqnarray*}
 E(Y^2) & = & \int_{0}^{\infty} y^2 f_Y(y)dy = \int_0^{\infty} y^2 \cdot \lambda e^{-\lambda y}dy\\
 &\stackrel{part. int.}{=}& -\left[y^2 \cdot e^{-\lambda y}\right]_0^{\infty} + \int_0^{\infty} 2y \cdot e^{-\lambda y}dy
 \\
 & = & 0 + 2 \frac{1}{\lambda} \int_0^{\infty} y \cdot \lambda e^{-\lambda y}dy \stackrel{E(Y)=1/\lambda}{=} \frac{2}{\lambda^2}
\end{eqnarray*}

And, again, using the result for the expectation one gets:
\[\ Var(Y)=\frac{2}{\lambda^2}-\frac{1}{\lambda^2} =\frac{1}{\lambda^2}\]\

\end{loesung}

\item[(b)] Derive the moment generating functions for both variables using the definition from the lecture. Why is it for the computation of moments sufficient to stick to $t < \lambda$? Why is this observation useful in the case of $Y$?

\begin{loesung}\vspace{0.2cm}
We start with the binomial random variable:

\begin{eqnarray*}
 M_X(t) & = & E(e^{tX}) = E(e^{t \sum_{i=1}^n X_i})\\
 & = & E(\prod_{i=1}^n e^{tX_i}) \stackrel{iid}{=} \prod_{i=1}^n E(e^{tX_i})\\
 & = & (1 \cdot (1-p) + e^t \cdot p)^n=(1-p+pe^t)^n
\end{eqnarray*}

For the exponential distributed variable we get the following results:
\begin{eqnarray*}
 M_Y(t) & = & E(e^{tY}) = \int_0^{\infty} e^{ty} \cdot \lambda e^{-\lambda y}dy\\
 & = & \lambda \int_0^{\infty} e^{y(t-\lambda )}dy = \frac{\lambda}{t-\lambda} \left[ e^{y(t-\lambda )} \right]_0^{\infty} \\
 & \stackrel{t<\lambda}{=} &  \frac{\lambda}{\lambda-t}  \\
\end{eqnarray*}
For $t \geq \lambda $ the moment generation function is not defined for $Y$. However the existence for small values is sufficient for the computation of the moments, as we take the derivative at zero: \[\  \frac{\partial^k}{(\partial t)^k}M_X(t)\bigg|_{t=0} = E(X^k) \]\  
\end{loesung}

\item[(c)] Validate the results from (a) using the derived moment generating functions.

\begin{loesung}\vspace{0.2cm}
We use the formula given above:

\begin{eqnarray*}
 E(X) & = & \frac{\partial}{(\partial t)}M_X(t)\bigg|_{t=0} = \frac{\partial}{(\partial t)}(1-p+pe^t)^n\bigg|_{t=0}\\
 & = & n(1-p+pe^t)^{n-1} \cdot p e^t|_{t=0}=np\\
\end{eqnarray*}

And for the second moment:

\begin{eqnarray*}
 E(X^2) & = & \frac{\partial^2}{(\partial^2 t)}M_X(t)\bigg|_{t=0} = \frac{\partial^2}{(\partial^2 t)}(1-p+pe^t)^n\bigg|_{t=0}\\
 & = & \frac{\partial}{\partial t}n(1-p+pe^t)^{n-1} \cdot p e^t|_{t=0}\\
 & = & n(n-1)(1-p+pe^t)^{n-2} \cdot p^2 e^{2t} + n(1-p+pe^t)^{n-1} \cdot p e^t|_{t=0}=n(n-1)p^2+np\\
\end{eqnarray*}
With the result for the first moment we get:
\[\ Var(Y) = E(X^2)-E(X)^2 = n^2p^2-np^2+np-n^2p^2 = np(1-p)\]\

For $Y$ one can compute the moments the following way:
\begin{eqnarray*}
 E(Y) & = & \frac{\partial}{(\partial t)}M_Y(t)\bigg|_{t=0} = \frac{\partial}{(\partial t)}\frac{\lambda}{\lambda-t}\bigg|_{t=0}\\
 & = & \frac{\lambda}{(\lambda-t)^2}\bigg|_{t=0}=\frac{1}{\lambda}\\
\end{eqnarray*}

\begin{eqnarray*}
 E(Y^2) & = & \frac{\partial^2}{(\partial^2 t)}M_Y(t)\bigg|_{t=0} = \frac{\partial^2}{(\partial^2 t)}\frac{\lambda}{\lambda-t}\bigg|_{t=0}\\
 & = & \frac{\partial}{\partial t}\frac{\lambda}{(\lambda-t)^2} \bigg|_{t=0}=\frac{2\lambda}{(\lambda-t)^3}\bigg|_{t=0}=\frac{2}{\lambda^2}\\
\end{eqnarray*}

And we get for the variance:
\[\ Var(Y) = E(X^2)-E(X)^2 = \frac{2}{\lambda^2}-\frac{1}{\lambda^2}=\frac{1}{\lambda^2}\]\
\end{loesung}
\ei







% \section*{Homework Problems}

% {\bf Exercise 1.5 Dependent Random Variables}\vspace{0.2cm}

% Given are two random variables $X$ and $Y$ with the joint density:
% \[
% f_{X,Y}(x,y)=\left\{
% \begin{array}{cl}
% c\cdot(x+y+xy) & ,0\leq x \leq1,\,0\leq y \leq 1, \\
% 0              & ,\mbox{Otherwise}.
% \end{array}
% \right.
% \]
% \bi
% \item[(a)] Give a short explanation why both variables are not independent.

% \begin{loesung}\vspace{0.2cm}
% The joint density function cannot be factorized into a term solely depending on $X$ and one solely depending on $Y$. Hence both are dependent.
% \end{loesung}

% \item[(b)] Calculate the value of $c$.

% \begin{loesung}\vspace{0.2cm}
%  As $f_{X,Y}$ is a density function, its integral must be one.
% \[
%   1 = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{X,Y}(x,y)dxdy
% \]
% Plugging in the given function:
% \begin{eqnarray*}
%   1 & = & \int_0^1\int_0^1 c \cdot (x+y+xy)dxdy \\
%     & = & c \cdot \int_0^1  \left[\frac{1}{2}x^2+xy+\frac{1}{2}x^2y\right]_0^1 dy \\
%     & = & c \cdot \int_0^1 \left(\frac{1}{2}+y+\frac{1}{2}y\right) dy \\
%     & = & c \cdot  \left[\frac{1}{2}y+\frac{1}{2}y^2+\frac{1}{4}y^2\right]_0^1  \\
%     & = & c \cdot  \frac{5}{4} \Longrightarrow c = 0.8.  \\
% \end{eqnarray*}
% Because $c > 0$ and $0 \leq x,y \leq 1$ $f_{X,Y}(x,y)\geq 0$ is fulfilled.
% \end{loesung}


% \item[(c)] Compute the marginal densities of $X$ and $Y$.

% \begin{loesung}\vspace{0.2cm}
% For $0 \leq x \leq 1$ the marginal density of $X$ takes the following form:
% \begin{eqnarray*}
%  f_{X}(x) & = & \int_{-\infty}^{\infty} f_{X,Y}(x,y) dy \\
%           & = & \int_0^1 0.8 \cdot (x+y+xy) dy \\
%           & = & 0.8 \cdot \left[xy + \frac{1}{2}y^2+\frac{1}{2}y^2x\right]_0^1 \\
%           & = & 0.8 \cdot \left( \frac{3}{2}x + \frac{1}{2}\right) = 0.4 \cdot (3x + 1).
% \end{eqnarray*}
% Otherwise the density is zero.\\
% Analogously, for the random variable $Y$ and $0 \leq y \leq 1$:
% \begin{eqnarray*}
%  f_{Y}(y) & = & \int_0^1 0.8 \cdot (x+y+xy) dx \\
%           & = & 0.8 \cdot \left[\frac{1}{2}x^2+xy+\frac{1}{2}x^2y\right]_0^1 \\
%           & = & 0.4 \cdot \left( 3y+1 \right).
% \end{eqnarray*}
% Again the density is zero otherwise.
% \end{loesung}


% \item[(d)] Derive the conditional densities $f_{X|Y}(x|y)$ and $f_{Y|X}(y|x)$.

% \begin{loesung}\vspace{0.2cm}
% For the conditional density of $X$ under the condition $Y = y$ and $0 \leq x,y \leq 1$, holds the following:

% \[
%   f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y (y)} = \frac{0.8 \cdot (x+y+xy)}{0.4 \cdot (3y+1)} = 2 \frac{x+y+xy}{3y+1}.
% \]
% Analogously, for the conditional density of $Y$ given $X = x$ and with $0 \leq x,y \leq 1$:
% \[
%   f_{Y|X}(y|x) = \frac{f_{X,Y}(x,y)}{f_X (x)} = \frac{0.8 \cdot (x+y+xy)}{0.4 \cdot (3x+1)} = 2 \frac{x+y+xy}{3x+1}.
% \]
% Both densities are zero otherwise.
% \end{loesung}


% \item[(e)] Calculate the covariance between $X$ and $Y$.

% \begin{loesung}\vspace{0.2cm}
% \[
%   Cov (X, Y) = E(XY) - E(X)E(Y).
% \]
% Taking the densities from (c) one can compute the expected values:
% \begin{eqnarray*}
%  E(X) & = & \int_{-\infty}^{\infty} x f_X(x)dx = \int_0^1 0.4x (3x+1)dx = 0.4 \left[x^3+\frac{1}{2}x^2\right]_0^1 = 0.6
%  \\
%  E(Y) & = & \int_{-\infty}^{\infty} y f_Y(y)dy = \int_0^1 0.4y (3y+1)dy = 0.4 \left[y^3+\frac{1}{2}y^2\right]_0^1 = 0.6
%  \\
%  E(XY) & = & \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} xy f_{X,Y}(x,y)dx dy = \int_0^1 \int_0^1 xy \cdot 0.8 \cdot (x+y+xy) dxdy  \\
%        & = &  0.8 \cdot \int_0^1 \int_0^1 (x^2y+xy^2+x^2y^2) dxdy \\
%        & = &  0.8 \cdot \int_0^1  \left[\frac{1}{3}x^3y+\frac{1}{2}x^2y^2+\frac{1}{3}x^3y^2\right]_0^1 dy \\
%        & = &  0.8 \cdot \int_0^1 \left(\frac{1}{3}y+\frac{1}{2}y^2+\frac{1}{3}y^2\right) dy \\
%        & = &  0.8  \left[\frac{1}{6}y^2+\frac{5}{18}y^3\right]_0^1 = 0.8 \cdot \frac{4}{9} \\
%        & = & \frac{16}{45}
% \end{eqnarray*}
% and therefore \[
%   Cov (X,Y) = \frac{16}{45} - \left(\frac{3}{5}\right)^2 = - \frac{1}{225}.
% \]
% \end{loesung}


% \item[(f)] Derive the joint distribution function $F_{X,Y}(x,y)$.

% \begin{loesung}\vspace{0.2cm}
% \begin{eqnarray*}
% P(X\leq x, Y\leq y)                 & = & \int_0^x \int_0^y 0.8 (u+v+uv)dvdu \\
%                                     & = & 0.8 \int_0^x \left[ uv + \frac{1}{2}v^2+\frac{1}{2}v^2u\right]_{v=0}^{v=y}du \\
%                                     & = & 0.8 \int_0^x \left( uy + \frac{1}{2}y^2+\frac{1}{2}y^2u \right)du \\
%                                     & = & 0.8 \left[\frac{1}{2}yu^2+\frac{1}{2}y^2u + \frac{1}{4}y^2u^2\right]_{u =
%                                     0}^{u = x} \\
%                                     & = & 0.8 \left(\frac{1}{2}x^2y+\frac{1}{2}xy^2+\frac{1}{4}x^2y^2\right) \\
%                                     & = & 0.4 xy \left(x+y+\frac{1}{2}xy\right).
% \end{eqnarray*}
% As a result:
% \[
%   F_{X,Y}(x,y) = \left\{ \begin{array}{cl}
%                      0, & \textrm{f\"{u}r} \: x \lor y < 0, \\
%                      0.4xy(x+y+\frac{1}{2}xy), & \textrm{f\"{u}r} \: x,y \in [0,1], \\
%                      F_X(x), & \textrm{f\"{u}r} \: x \in [0,1] \wedge y > 1, \\
%                      F_Y(y), & \textrm{f\"{u}r} \: y \in [0,1] \wedge x > 1, \\
%                      1, & \textrm{f\"{u}r} \: x,y > 1. \\
%                    \end{array}\right.
% \]
% \end{loesung}


% \ei





\vspace{\fill}
\input{../Fuss}


\end{document}





%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
